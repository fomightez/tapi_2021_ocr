{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images to Text: A Gentle Introduction to Optical Character Recognition with PyTesseract\n",
    "\n",
    "***Lesson 02: OCR as a Process*** \n",
    "\n",
    "Wednesday, June 16 2021 - [13:00-14:30 UTC](https://savvytime.com/converter/utc/jun-16-2021/13-30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Review of Lesson 01](#lesson-01)\n",
    "- [Lesson 02: OCR as a Process](#lesson-02)\n",
    "    - [Choosing OCR](#choosing)\n",
    "    - [Preparing a Corpus](#preparing)\n",
    "    - [Pre-Processing](#pre-processing)\n",
    "    - [Performing OCR](#performing)\n",
    "    - [Post-Processing](#post-processing)\n",
    "- [Homework](#homework)\n",
    "- [Resources](01-WhatIsOCR.ipynb#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>By the end of this lesson, participants should be able to</strong>\n",
    "    <ul>\n",
    "        <li>describe and implement an OCR workflow including pre- and post-processing steps;</li>\n",
    "        <li>explain the importance of performing adjustments (pre-processing) to inputs before running OCR;</li>\n",
    "        <li>identify possible technical challenges presented by specific texts and propose potential solutions;</li>\n",
    "        <li>assess the degree of accuracy they have achieved in performing OCR.</li>   \n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Lesson 01: What is OCR? Why is it important?<a class=\"anchor\" id=\"lesson-01\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What we covered in [Lesson 01](01-WhatIsOCR.ipynb)\n",
    "- Slack Discussion\n",
    "    - [How to Install & Run Jupyter Notebooks Locally](04-Installing%2BRunningLocally.ipynb)\n",
    "- New Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 02: OCR Options & as a Process <a class=\"anchor\" id=\"lesson-02\"></a>\n",
    "---\n",
    "\n",
    "**Before we get started, we'll need to reinstall Tesseract and import PIL and Pytesseract.** Run each script below, waiting for the first to finish before running the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tesseract on Binder.\n",
    "# The exclamation runs the command as a terminal command.\n",
    "# This may take 1-2 minutes.\n",
    "# Source: Nathan Kelber & JStor Labs Constellate team.\n",
    "!conda install -c conda-forge -y tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Image and ImageOps modules \n",
    "# from the Pillow Library, which will help us access the image.\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "\n",
    "# Import the pytesseract library, which will run the OCR process.\n",
    "import pytesseract\n",
    "\n",
    "print('Modules successfully installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [From Lesson 01](01-WhatIsOCR.ipynb#variations) -  Variations on a Theme: Tesseract's Options\n",
    "\n",
    "Tesseract offers a number of different modes, or settings, that we can use to customize output. There are two types of modes: OEMs (OCR Engine Modes), which specify which OCR tools are available to Tesseract to use, and PSMs (Page Segmentations Modes), which specify how the OCR tools should read the image files--how to separate and order sections of text in the image file.\n",
    "\n",
    "#### OCR Engine Modes (OEMs)\n",
    "\n",
    "Run the following command to view the list of OEMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tesseract --help-oem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following script, trying each of the different OEMs in turn replace the number (X) in the first line to change the OEM: \n",
    "\n",
    "`custom_oem_config = r'--oem X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the OEM number below to try\n",
    "# running another OCR mode.\n",
    "# 3 is the default setting.\n",
    "custom_oem_config = r'--oem X'\n",
    "\n",
    "# Open a specific image file, convert the text in the image to computer-readable text (OCR)\n",
    "# following the language and mode configuration we specify,\n",
    "# and then print the results for us to see here.\n",
    "print(pytesseract.image_to_string(Image.open(\"sessionlawsresol1955nort_0057.jpg\"), lang=\"eng\", config=custom_oem_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What did you notice about the different modes? How did they differ from one another?*\n",
    "\n",
    "Here's more of an explanation of OCR Engine Modes (OEMs):\n",
    "\n",
    "- *0 - Original Tesseract only.* - This mode runs only the main Tesseract mode.\n",
    "  \n",
    "- *1 - Cube only.* - This mode runs only Cube, [according to Google](https://code.google.com/archive/p/tesseract-ocr-extradocs/wikis/Cube.wiki), \"an alternative recognition mode for Tesseract. It is slower than the original recognition engine, but often produces better results.\" [A Nanonets tutorial explains](https://nanonets.com/blog/ocr-with-tesseract/) that this is the LSTM mode. There is not much documentation out about this.\n",
    "  \n",
    "- *2 - Tesseract + Cube.* - Both Tesseract (Nanonets refers to this as \"Legacy\") and Cube (LSTM) modes are used.\n",
    "\n",
    "- *3 - Default, based on what is available.* - Tesseract will choose an OEM based on the configurations (language, PSM) we give it. Even if we don't include the configuration information, Tesseract will run in OEM 3.\n",
    "\n",
    "#### Page Segmentation Modes (PSM)\n",
    "\n",
    "Run the following command to view all of the PSMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tesseract --help-psm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, our configuration looks like\n",
    "\n",
    "`custom_oem_config = r'--psm X'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the PSM number below to try\n",
    "# running another page segmentation mode.\n",
    "# 3 is the default setting.\n",
    "custom_psm_config = r'--psm 3'\n",
    "\n",
    "# Open a specific image file, convert the text in the image to computer-readable text (OCR)\n",
    "# following the language and mode configuration we specify,\n",
    "# and then print the results for us to see here.\n",
    "print(pytesseract.image_to_string(Image.open(\"sessionlawsresol1955nort_0057.jpg\"), lang=\"eng\", config=custom_psm_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the PSMs are meant for images that have little text in them -- such as images that include road or store signs. [See Tesseract's documentation on improving OCR quality.](https://tesseract-ocr.github.io/tessdoc/ImproveQuality)\n",
    "\n",
    "**Most of the time, the default OEM and PSM is best.** There may be times when you are working with materials for which experimenting with these options may be useful.\n",
    "\n",
    "Note that it's possible to customize the `oem` and `psm` together. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the numbers below to try\n",
    "# running other modes together.\n",
    "custom_oem_psm_config = r'--oem 3 --psm 4'\n",
    "\n",
    "# Open a specific image file, convert the text in the image to computer-readable text (OCR)\n",
    "# following the language and mode configuration we specify,\n",
    "# and then print the results for us to see here.\n",
    "print(pytesseract.image_to_string(Image.open(\"sessionlawsresol1955nort_0057.jpg\"), lang=\"eng\", config=custom_oem_psm_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Formats\n",
    "\n",
    "In addition to .txt, Tesseract can convert OCR'ed images into [hOCR (HTML)](https://en.wikipedia.org/wiki/HOCR), searchable PDF, and TSV.\n",
    "\n",
    "*Exercise*\n",
    "\n",
    "The scripts below output various file formats. Try each and then click the file link below each script to view the output. You'll also find the files by clicking on the Jupyter icon at the top of this window.\n",
    "\n",
    "1. Text: Note that this script is more detailed than our initial `print(pytesseract.image_to_string(Image.open(\"sessionlawsresol1955nort_0057.jpg\"), lang=\"eng\"))`. Read through the comments (#) below to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the image file. (Assign it to a variable.)\n",
    "# You can change the filename in quotes below to OCR a different file.\n",
    "file = \"sessionlawsresol1955nort_0057.jpg\"\n",
    "\n",
    "# Open the file named above. \n",
    "# While it's open, do several things:\n",
    "with open(file, 'rb') as inputFile:\n",
    "        \n",
    "    # Read the file using PIL's Image module.\n",
    "    img = Image.open(inputFile)\n",
    "    \n",
    "    # Run OCR on the open file.\n",
    "    ocrText = pytesseract.image_to_string(img)\n",
    "        \n",
    "    # Get a file name--without the extension-- \n",
    "    # to use when we name the output file.\n",
    "    fileName = file.strip('.jpg')\n",
    "\n",
    "# The image file above will be closed before moving on to this line.\n",
    "# The OCR'ed text has been pulled from the image and stored in\n",
    "# a Python variable for us to continue to use.\n",
    "\n",
    "# Create and open a new text file, name it to match its input file,\n",
    "# declare its encoding to be UTF-8 so that it correctly outputs\n",
    "# non-ASCII characters.\n",
    "with open(fileName + \".txt\", \"w\", encoding=\"utf-8\") as outFile:\n",
    "        \n",
    "    # and write the OCR'ed text to the file.\n",
    "    outFile.write(ocrText)\n",
    "\n",
    "# Display a message to let us know the file has been created\n",
    "# and the script successfully completed.\n",
    "print(fileName, \"text file successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open [sessionlawsresol1955nort_0057.txt](sessionlawsresol1955nort_0057.txt) to see the results.\n",
    "\n",
    "2. PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the image file. (Assign it to a variable.)\n",
    "# You can change the filename in quotes below to OCR a different file.\n",
    "file = \"sessionlawsresol1955nort_0057.jpg\"\n",
    "\n",
    "# Get a file name--without the extension-- \n",
    "# to use when we name the output file.\n",
    "fileName = file.strip('.jpg')\n",
    "\n",
    "# Run OCR on an image file and save it as a PDF object (not file)\n",
    "# within Python.\n",
    "pdf = pytesseract.image_to_pdf_or_hocr(file, extension='pdf')\n",
    "\n",
    "# Create a new empty pdf.\n",
    "with open(fileName + \".pdf\", 'w+b') as f:\n",
    "    \n",
    "    # Save the PDF object to the new empty PDF file.\n",
    "    f.write(pdf)\n",
    "\n",
    "# Display a message to let us know the file has been created\n",
    "# and the script successfully completed.\n",
    "print(fileName, \"PDF successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open [sessionlawsresol1955nort_0057.pdf](sessionlawsresol1955nort_0057.pdf) to see the results. *What do you notice about this PDF?* Try running a search within file (Command+F or Control+F to open the Find search box)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the image file. (Assign it to a variable.)\n",
    "# You can change the filename in quotes below to OCR a different file.\n",
    "file = \"sessionlawsresol1955nort_0057.jpg\"\n",
    "\n",
    "# Get a file name--without the extension-- \n",
    "# to use when we name the output file.\n",
    "fileName = file.strip('.jpg')\n",
    "\n",
    "# Run OCR on an image file and save it as an HTML object (not file)\n",
    "# within Python.\n",
    "hocr = pytesseract.image_to_pdf_or_hocr(file, extension='hocr')\n",
    "\n",
    "# Create a new empty HTML file. Open it in \"w+b\" mode.\n",
    "# \"w+b\" is a mode that tells Python to write whatever\n",
    "# data we give to a file in binary mode--meaning that \n",
    "# it will not apply any encoding or try to translate\n",
    "# a non-ASCII character to an ASCII character.\n",
    "with open(fileName + \".html\", 'w+b') as f:\n",
    "    \n",
    "    # Save the PDF object to the new empty PDF file.\n",
    "    f.write(hocr)\n",
    "\n",
    "# Display a message to let us know the file has been created\n",
    "# and the script successfully completed.\n",
    "print(fileName, \"HTML successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open [sessionlawsresol1955nort_0057.html](sessionlawsresol1955nort_0057.html) to see the results. *What do you notice about this HTML file?* If you wish, save it to your Desktop and open in a text editor to view the HTML syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Languages\n",
    "\n",
    "If we do not include `lang=\"eng\"` when we run the above code, Tesseract will *assume* English. Run the following to get a list of all the language codes. [A table of these is available here.](https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a list of languages in their 3-letter codes supported by Tesseract.\n",
    "print(pytesseract.get_languages(config=''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercises*\n",
    "\n",
    "Try OCR'ing [this file](faust.png) ([Source](https://archive.org/details/fausteinetragodi00goet/page/n7/mode/2up)). Change the [3-letter language code](https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html) to match the language in that document. *To do this, you will need to modify the code below to include the correct file name and language code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a specific image file, convert the text in the image to computer-readable text (OCR),\n",
    "# and then print the results for us to see here.\n",
    "\n",
    "# REPLACE THE FILE NAME with one of the sample files above. (faust.png)\n",
    "# REPLACE THE LANGUAGE attribute with the correct language code(s). (deu)\n",
    "print(pytesseract.image_to_string(Image.open(\"REPLACE-THIS-FILE-NAME.jpg\"), lang=\"lan\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try OCR'ing [this file](bible.png) ([Source](https://archive.org/details/holybibleinhindi00alla)), which includes [multiple languages](). The syntax will be `lang=\"lan+gua\"` -- replace `lan` and `gua` with the correct language codes. *The first language will be the \"primary\" language. Try changing the order of the languages to see how the output changes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Open a specific image file, convert the text in the image to computer-readable text (OCR),\n",
    "# and then print the results for us to see here.\n",
    "\n",
    "# REPLACE THE FILE NAME with one of the sample files above. (bible.png)\n",
    "# REPLACE THE LANGUAGE attribute with the correct language code(s).\n",
    "print(pytesseract.image_to_string(Image.open(\"REPLACE-THIS-FILE-NAME.png\"), lang=\"lan+gua\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 01 Homework Review & Practice <a class=\"anchor\" id=\"homework-lesson01\"></a>\n",
    "---\n",
    "\n",
    "*As you work through the following steps, discuss your observations with your small group: what do you notice about the OCR output? How does it vary for different sources/settings? Why do you think the output varies for different sources/settings? What might you need to do to improve the OCR output?*\n",
    "\n",
    "Use the following code blocks to try OCR'ing various texts. You could use your own files containing digitized texts or locate files to try via [JStor](https://www.jstor.org/), the [Internet Archive](https://archive.org/), [Chronicling America](https://chroniclingamerica.loc.gov/) or other resources. Try texts in different languages, fonts or types, formats, layouts, etc. \n",
    "\n",
    "### 1. Upload your selected text(s) to a new folder in your space on Binder:\n",
    "\n",
    "- Make sure that the texts you select are stored in an image (.jpg, .png, .tiff) format. If you have selected a text with multiple pages, make sure each page is stored in a separate file. *If you have PDF files and are not sure how to generate images from them, bring them to Lesson 02. We'll be looking at how to generate image files together during the lesson.*\n",
    "\n",
    "- Click the \"Jupyter\" icon at the top of the browser. (Recommended: right click and select \"Open in New Tab.\")\n",
    "\n",
    "- Above the list of files in Jupyter at the top right, click \"New\" and create a new folder.\n",
    "\n",
    "- A new \"Untitled Folder\" will be created. Select the check box to the left of your new folder.\n",
    "\n",
    "- Click \"Rename\" at the top left and give your folder a name.\n",
    "\n",
    "- Click your new folder's name in the list of files and folders.\n",
    "\n",
    "- In your new folder, select \"Upload\" in the top right and upload your chosen image file(s).\n",
    "\n",
    "### 2. Perform OCR on your image file. \n",
    "\n",
    "Use the code blocks above or start fresh below. Change the language attribute to match the text's language. Try out the various settings we looked at above.\n",
    "\n",
    "Below, make sure to replace \"FOLDER NAME/FILE NAME\" with your folder name and specific file name. For example, `sample/sessionlawsresol1955nort_0057.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the image file. (Assign it to a variable.)\n",
    "# REPLACE THE FOLDER AND FILE NAME BELOW.\n",
    "file = \"FOLDER_NAME/FILE_NAME.jpg\"\n",
    "\n",
    "custom_oem_psm_config = r'--oem 3 --psm 3'\n",
    "\n",
    "# Open the file named above. \n",
    "# While it's open, do several things:\n",
    "with open(file, 'rb') as inputFile:\n",
    "        \n",
    "    # Read the file using PIL's Image module.\n",
    "    img = Image.open(inputFile)\n",
    "    \n",
    "    # Run OCR on the open file.\n",
    "    ocrText = pytesseract.image_to_string(img, lang=\"eng\", config=custom_oem_psm_config)\n",
    "        \n",
    "    # Get a file name--without the extension-- \n",
    "    # to use when we name the output file.\n",
    "    fileName = file.strip('.jpg')\n",
    "\n",
    "# The image file above will be closed before moving on to this line.\n",
    "# The OCR'ed text has been pulled from the image and stored in\n",
    "# a Python variable for us to continue to use.\n",
    "\n",
    "# Create and open a new text file, name it to match its input file,\n",
    "# declare its encoding to be UTF-8 so that it correctly outputs\n",
    "# non-ASCII characters,\n",
    "with open(fileName + \".txt\", \"w\", encoding=\"utf-8\") as outFile:\n",
    "        \n",
    "    # and write the OCR'ed text to the file.\n",
    "    outFile.write(ocrText)\n",
    "\n",
    "# Display a message to let us know the file has been created\n",
    "# and the script successfully completed.\n",
    "print(fileName, \"text file successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Repeat steps 1-2 with other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BREAK\n",
    "\n",
    "<img src=\"images/noun_Cafe_3166430.png\" width=\"20%\" alt=\"A coffee cup on a saucer with steam rising from the cup.\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR as a Process\n",
    "\n",
    "<img src=\"images/noun_arrow with loops_2073885.png\" width=\"20%\" style=\"float:right;\" alt=\"arrow with loops by Kalinin Ilya from the Noun Project\" title=\"arrow with loops by Kalinin Ilya from the Noun Project\" />\n",
    "\n",
    "*The main example for today's exercises is drawn from the [On the Books: Algorithms of Resistance](https://onthebooks.lib.unc.edu/) project.*\n",
    "\n",
    "**Producing OCR'ed text is an iterative, rather than a linear, process.** To get the best possible output involves multiple steps and in some instances repetition of steps. Here's an overview of what these steps can look like. \n",
    "\n",
    "- [Choosing OCR](#choosing)\n",
    "- [Preparing a Corpus](#preparing)\n",
    "- [Preparing to OCR (Pre-Processing)](#pre-processing)\n",
    "- [Performing OCR](#performing)\n",
    "- [Refining or \"Cleaning\" OCR (Post-Processing)](#post-processing)\n",
    "\n",
    "*Keep in mind that this process can vary not only based on the complexity and legibility of your corpus but also based on the resources you have. Consider your expertise, whether you are working with a team or by yourself, how much time you have, and your ultimate research goal. These should all factor in to how complex you make your OCR process.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing OCR <a class=\"anchor\" id=\"choosing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <p>There are <strong>multiple possible ways</strong> to approach converting image files <em>showing</em> text that humans can read to text files <em>containing</em> computer readable text.</p>\n",
    "    <p>Some questions to consider when preparing to perform OCR:</p>\n",
    "   <ol>\n",
    "       <li>How much text do I need to convert?</li>\n",
    "       <li>Is the text born-digital or digitized from paper or another analog physical medium?</li>\n",
    "       <li>Is the text written by hand or printed using a press?</li>\n",
    "       <li>How is the text formatted on the page?</li>\n",
    "       <li>Is the digitized text showing signs of damage, such as fading, spills, smears, or paper disintigration or tearing?</li>\n",
    "       <li>Is the text using a historical script such as <a href=\"https://en.wikipedia.org/wiki/Carolingian_minuscule\" alt=\"Wikipedia page describing Carolingian miniscule\">Carolingian miniscule</a>?</li>\n",
    "       <li>Is the text in a human language that computers can \"read\"?</li>\n",
    "    </ol>\n",
    "    <p>Some of these questions may not be relevant to you, but they are worth being aware of. We'll go through each briefly below.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How much text?\n",
    "\n",
    "We begin with this question because if you have only a few pages, there may be merit in typing them out by hand in a text editor, and perhaps working with a team to do so. If you have hundreds of thousands of pages, though, it may take far longer than you have time, even working with a team, to manually transcribe every page you need to complete a project. That may mean that you'll want to start with an automated transcription (OCR) process and then work to correct what the computer outputs. *There are caveats to this method--read on.*\n",
    "\n",
    "#### 2. Is the text born-digital or digitized from paper or another analog physical medium?\n",
    "\n",
    "*Born-digital texts in PDF and image formats can be easier for a computer to \"read\" than are scanned documents,* even if the scanners use the highest resolution equipment. This is particularly true of older printed texts for reasons that we'll learn more about below.\n",
    "\n",
    "*An exception to this is if a born-digital text is stored in an image or other non-text-editable format that is uncommon, proprietary, or outdated.* Then computers may have a hard time accessing the file in order to parse the text contained. (So always save documents in an interoperable--can be opened by different software programs--file format either as [editable text](https://www.archives.gov/records-mgmt/policy/transfer-guidance-tables.html#textualdata) or as [non-editable image or archival document--PDF--formats](https://www.archives.gov/records-mgmt/policy/transfer-guidance-tables.html#scannedtext).)\n",
    "\n",
    "#### 3. Is the text written by hand or printed using a press?\n",
    "\n",
    "OCR technologies were *initially developed to deal only with digitized texts created using a [printing press](https://en.wikipedia.org/wiki/Printing_press)*. This was because printing presses offer a certain amount of consistency in typeface, font, and layout that programmers could use to create rules for computers to follow (algorithms!). Meanwhile, handwriting is, by and large, more individualistic and inconsistent. Most programs for OCR still focus only on printed texts, but *there are a growing number of projects and toolkits now available for what's called variously [\"digital paleography\"](https://academic.oup.com/dsh/article/32/suppl_2/ii89/4259068), [\"handwriting recognition\" (HWR)](https://en.wikipedia.org/wiki/Handwriting_recognition), and [\"handwritten text recognition\" (HTR)](https://en.wikipedia.org/wiki/Handwriting_recognition). [Transkribus](https://readcoop.eu/transkribus/) is a good tool to start with.*\n",
    "\n",
    "As an example, let's compare excerpts from Toni Morrison's *Beloved*. The first image below is a page from an early draft, written in Morrison's own hand on a legal pad. The second image is a segment from a digitized print version. These are not the same passages, but they are noticably different in how we read them: Try reading each. What's different about the experience--think about order of reading, ease of reading, and any other differences that come to mind:\n",
    "\n",
    "<img src=\"images/07-ocr-03.jpeg\" width=\"p0%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"A page from Toni Morrison's early draft of Beloved. Courtesy of Princeton University Library\" title=\"A page from Toni Morrison's early draft of Beloved. Courtesy of Princeton University Library\" />\n",
    "\n",
    "An early draft of Toni Morrison's *Beloved*. Image credit: [Princeton University Library](https://blogs.princeton.edu/manuscripts/2016/06/07/toni-morrison-papers-open-for-research/)\n",
    "\n",
    "<img src=\"images/07-ocr-02.jpeg\" width=\"90%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screenshot of a page in Toni Morrison's Beloved. Preview hosted on Google Books.\" title=\"Screenshot of a page in Toni Morrison's Beloved. Preview hosted on Google Books.\" />\n",
    "\n",
    "Screenshot from a digitized version of the published *Beloved*, available in [Google Books](https://www.google.com/books/edition/Beloved/sfmp6gjZGP8C?hl=en&gbpv=1&dq=toni+morrison+beloved&printsec=frontcover).\n",
    "\n",
    "#### 4. How is the text formatted on the page?\n",
    "\n",
    "*Look at the texts above again: How are they formatted similarly or differently?* While both use a left-to-right writing system, the printed version appears in a single column that is evenly spaced both horizontally and vertically. The manuscript text appears on lined paper in a single column, but it includes a number of corrections written between lines or even in different directions (vertically) on the page. You might have tilted your head to read some of that text--if you had been holding the paper in your hands, you might have turned the paper 90 degrees. But computers don't necessarily know to do that (yet). They need a predictable pattern to follow, which the printed text provides.\n",
    "\n",
    "That said, not all historical printings are as regular as this *Beloved* excerpt. Let's take a look at one more example from *On The Books*:\n",
    "\n",
    "<img src=\"images/07-ocr-04.jpeg\" width=\"70%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screenshot from the 1887 North Carolina session laws digitized by UNC Libraries and shared via the Internet Archive.\" title=\"Screenshot from the 1887 North Carolina session laws digitized by UNC Libraries and shared via the Internet Archive.\" />\n",
    "\n",
    "Like the printed *Beloved* example, this selection from the [1887 North Carolina session laws](https://archive.org/details/lawsresolutionso1887nort/page/776/mode/2up) was created using a printing press and with mostly even vertical spacing between lines that run left to right. However, in addition to the changing typeface, there is in addition to the main column of text a much smaller column of annotations--[\"marginalia\"](https://en.wikipedia.org/wiki/Marginalia)--created to aid readers who would have been looking for quick topical references rather than reading a volume from start to finish. These created a problem for the *On The Books* team because the computer read them as being part of the main text. What resulted (with other OCR errors removed) would have looked like:\n",
    "\n",
    "`SECTION 1. The Julian S. Carr, of Durham, North Carolina, Mar- Body politic. cellus E. McDowell, Samuel H. Austin, Jr., and John A. McDowell,`\n",
    "\n",
    "What's the problem here? The marginalia, `Body politic`, have been interspersed with the text as the computer \"reads\" all the way across the page. The line should read:\n",
    "\n",
    "`SECTION 1. The Julian S. Carr, of Durham, North Carolina, Mar-cellus E. McDowell, Samuel H. Austin, Jr., and John A. McDowell,`\n",
    "\n",
    "The computer doesn't realize that it's creating errors, and if the annotations are not in any way mispelled, the *On The Books* team might have a hard time finding and removing all of these insertions. The insertions might then have also caused major difficulties in future computational analyses.\n",
    "\n",
    "Because marginalia would have caused such havoc in their dataset, the *On The Books* team decided to remove the marginalia as part of preparing for OCR. You can [find the documentation about this in the team's Github](https://github.com/UNC-Libraries-data/OnTheBooks/tree/master/examples/marginalia_determination).\n",
    "\n",
    "#### 5. Is the digitized text showing signs of damage, such as fading, spills, smears, or paper disintigration or tearing?\n",
    "\n",
    "Even with the use of state of the art scanning equipment ([for example](https://www.digitalnc.org/about/what-we-use-to-digitize-materials/)), annotations on or damage to analog physical media can interfere with OCR. Here are some examples.\n",
    "\n",
    "*Someone writing on a printed text.* These check marks might be read as \"l\" or \"V\" by the computer:\n",
    "\n",
    "<img src=\"images/07-ocr-05.jpeg\" width=\"70%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screenshot of check marks written in 1887 North Carolina sessions law digitized by UNC Libraries and shared via the Internet Archive.\" title=\"Screenshot of check marks written in 1887 North Carolina sessions law digitized by UNC Libraries and shared via the Internet Archive.\" />\n",
    "\n",
    "`not be worked on said railroad in the counties of New l Hanover or Pender.`\n",
    "\n",
    "The *printed text has faded* so that individual characters are broken up, and the ink is harder to read. (Historic newpapers are notorious for this. [Here's an example](https://chroniclingamerica.loc.gov/lccn/sn85042104/1897-01-14/ed-1/seq-6/#date1=1890&index=2&rows=20&words=asylum+ASYLUM+Asylum&searchType=basic&sequence=0&state=North+Carolina&date2=1910&proxtext=asylum&y=0&x=0&dateFilterType=yearRange&page=1).):\n",
    "\n",
    "<img src=\"images/07-ocr-06.jpeg\" width=\"70%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screenshot of faded text printed in 1887 North Carolina sessions law digitized by UNC Libraries and shared via the Internet Archive.\" title=\"Screenshot of faded text printed in 1887 North Carolina sessions law digitized by UNC Libraries and shared via the Internet Archive.\" />\n",
    "\n",
    "`three hundred dollars' t\\\"Orth of property and the same arnouut`\n",
    "\n",
    "A *smudge, spot, or spill has appeared on the page*, causing the computer to misinterpret a character or eroneously add characters:\n",
    "\n",
    "<img src=\"images/07-ocr-06.jpeg\" width=\"70%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screenshot of spot on text in 1887 North Carolina sessions law digitized by UNC Libraries and shared via the Internet Archive.\" title=\"Screenshot of spot on text in 1887 North Carolina sessions law digitized by UNC Libraries and shared via the Internet Archive.\" />\n",
    "\n",
    "`a S€1'.)arate fund,`\n",
    "\n",
    "There is also one additional possibility that can be a result of close binding, or the human doing the scanning avoiding the possibility of breaking tight or damaged binding: that is, **text that is rotated slightly** on the digitized page so that it appears at a slight angle.\n",
    "\n",
    "<img src=\"images/07-ocr-08.jpeg\" width=\"70%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screenshot of tilted text in 1887 North Carolina sessions law digitized by UNC Libraries and shared via the Internet Archive.\" title=\"Screenshot of tilted text in 1887 North Carolina sessions law digitized by UNC Libraries and shared via the Internet Archive.\" />\n",
    "<br/>\n",
    "\n",
    "#### 6. Is the text using a historical script such as [Carolingian miniscule](https://en.wikipedia.org/wiki/Carolingian_minuscule)?\n",
    "\n",
    "This applies mainly to students and scholars working with *historical texts printed or written in scripts that are not commonly legible to humans (or computers) today*. These could be anything from medieval scripts like Carolingian miniscule to neogothic scripts used in [twentieth-century German-American newspapers](https://chroniclingamerica.loc.gov/lccn/sn84027107/1915-07-01/ed-1/seq-1/) to the many, many historic non-Western scripts. These are areas where research is in progress, but you might find this [Manuscript OCR](https://manuscriptocr.org/) tool of interest as well as this [essay on the challenges medievalists continue to face when using OCR technologies](http://digitalhumanities.org/dhq/vol/13/1/000412/000412.html). When choosing an OCR tool, this is one of the capabilities you'll want to check for.\n",
    "\n",
    "#### 7. Is the text in a human language that computers can \"read\"?\n",
    "\n",
    "Similar to the historic script issue, for scholars and students working with or studying *less common, perhaps endangered, and especially non-Western languages*, you'll want to see if an OCR tool supports your particular language. Tesseract offers [a list of the languages and scripts it supports](https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html). Tesseract supports 125 languages and dialects--likely those most commonly spoken, based on shared [writing systems](https://en.wikipedia.org/wiki/Writing_system), and/or those that researchers may have invested time in training Tesseract to \"read\" for some specific reason. This is just a fraction of the languages and scripts in the world, though. \n",
    "\n",
    "Unfortunately, if you're working with Indigenous writing systems such as [Canadian Aboriginal Syllabics](https://en.wikipedia.org/wiki/Canadian_Aboriginal_syllabics), you still may need to seek out additional support from computer scientists for developing OCR technologies to \"read\" these languages. This lack of support for many endangered languages is just one example of bias found in the broader technology industry.\n",
    "\n",
    "#### Other questions we should ask?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a Corpus <a class=\"anchor\" id=\"preparing\"></a>\n",
    "\n",
    "#### If it's not already, digitize your corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <p><strong>If you're doing the scanning yourself or will be working with someone to newly digitize materials,</strong> it's a good idea to carefully plan your scanning process. Every step matters in terms of generating the best possible OCR results. Digital NC have posted their <a href=\"https://www.digitalnc.org/policies/digitization-guidelines/\" alt=\"Digital NC digitization guidelines\">digitization guidelines</a> along with <a href=\"https://www.digitalnc.org/about/what-we-use-to-digitize-materials/\" alt=\"Digital NC scanning equipment\">descriptions of their scanning equipment</a>. These can provide a helpful starting point if you will be beginning your project with undigitized materials.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File formats\n",
    "\n",
    "As we discussed previously, Tesseract prefers images. If you are starting from a PDF or a bunch of PDFs, here's are a few ways you can convert each page into a separate image file:\n",
    "\n",
    "- [Use Adobe online](https://www.adobe.com/acrobat/online/pdf-to-jpg.html) (1 pdf at a time...)\n",
    "- [Use Adobe Acrobat](https://helpx.adobe.com/acrobat/using/exporting-pdfs-file-formats.html?mv=product) (1 pdf at a time...)\n",
    "- [Use pdf2image](https://pypi.org/project/pdf2image/) (1 pdf or many)\n",
    "- Other options...\n",
    "\n",
    "*Note:* Technically, it's possible to feed Tesseract a PDF, but breaking up a PDF into images breaks down the OCR process from one massive task into a bunch of smaller tasks that are better for your computer -- if something happens, and the process is interrupted, you'll be able to pick up from where you left off if you are working from images. If you are processing an entire PDF and your computer freezes, you'll need to start over from the beginning...\n",
    "\n",
    "***Here's how to convert a pdf to an image file using pdf2image:***\n",
    "\n",
    "First, we need to install a few new tools. Run each one at a time. Wait for each to finish before moving to the next script.\n",
    "\n",
    "Install Poppler, a dependency for pdf2image. Note that depending on where you are working, Poppler has different installation processes. [(See pdf2image documentation.)](https://pypi.org/project/pdf2image/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!conda install -c conda-forge -y poppler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Poppler is finished installing, run the following to install pdf2image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdf2image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If pdf2image is installed, we can run the following script to convert a pdf file into an image file. Note the folder organization we have created for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pdf2image's convert_from_path module.\n",
    "from pdf2image import convert_from_path\n",
    " \n",
    "# Get the PDF and convert to a group of PIL (Pillow) objects.\n",
    "# This does NOT save the images as files.\n",
    "images = convert_from_path('pdf2image_example/sample_pdf.pdf')\n",
    "\n",
    "# This step saves images as files:\n",
    "\n",
    "# For each PIL image object:\n",
    "for i in range(len(images)):\n",
    "    \n",
    "    # Remember the folder name where we want to store the images.\n",
    "    folder = 'pdf2image_example/'\n",
    "\n",
    "    # Create a file name that includes the folder, file name, and\n",
    "    # a file number, as well as the file extension.\n",
    "    fileName = folder + 'sample_pdf'+ str(i) +'.jpg'\n",
    "\n",
    "    # Save each PIL image object using the file name created above\n",
    "    # and declare the image's file format. (Try also PNG or TIFF.)\n",
    "    images[i].save(fileName, 'JPEG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the results in the [pdf2image_example](pdf2image_example) folder.\n",
    "\n",
    "We can also convert a bunch of pdfs at once. Again, notice the folder organization--a little more complex this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pdf2image's convert_from_path module.\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# Import os, a module for file management.\n",
    "import os\n",
    "\n",
    "# Import glob, a module that helps with file management.\n",
    "import glob\n",
    "\n",
    "# Open the file folder where our sample pages are stored.\n",
    "# Look only for the files ending with the \".jpg\" file extension.\n",
    "pdf_folder = glob.glob(\"pdf2image_example_pdfs/*.pdf\")\n",
    "\n",
    "# Get the name of the folder where we'll store all of the images.\n",
    "# We'll end up creating subfolders within this to keep each pdf's\n",
    "# output separate.\n",
    "parent_image_folder = 'pdf2image_example_jpgs'\n",
    "\n",
    "# For each pdf file in the pdf folder, do the following:\n",
    "for p in pdf_folder:\n",
    "\n",
    "    # Get the pdf's name and split the folder name from the file name.\n",
    "    # (pdf2image_example_pdfs/sample_0X.pdf)\n",
    "    pdf_name = p.split('/')\n",
    "    \n",
    "    # Get just the file name (sample_0X) and remove the file extension.\n",
    "    pdf_name = pdf_name[1].strip('.pdf')\n",
    "    \n",
    "    # Create a folder name for the images we'll create from this pdf.\n",
    "    image_folder_name = pdf_name + \"_jpgs\"  \n",
    "    \n",
    "    # Create the path for the new image folder, which is separate from \n",
    "    # the pdf folder. (pdf2image_example_jpgs/sample_0X_jpgs)\n",
    "    image_path = os.path.join(parent_image_folder, image_folder_name)\n",
    "\n",
    "    # Check whether the new image folder already exists.\n",
    "    if os.path.exists(image_path) == False:\n",
    "        \n",
    "        # If the folder does NOT exist, create it.\n",
    "        image_folder = os.mkdir(image_path)\n",
    "    \n",
    "    # Get the PDF and convert to a group of PIL (Pillow) objects.\n",
    "    # This does NOT save the images as files.\n",
    "    images = convert_from_path(p)\n",
    "    \n",
    "    # For each PIL image object:\n",
    "    for i in range(len(images)):\n",
    "\n",
    "        # Create a file name that includes the folder, file name, and\n",
    "        # a file number, as well as the file extension.\n",
    "        image_name = image_path + '/' + pdf_name + '_0' + str(i) +'.jpg'\n",
    "\n",
    "        # Save each PIL image object using the file name created above\n",
    "        # and declare the image's file format. (Try also PNG or TIFF.)\n",
    "        images[i].save(image_name, 'JPEG')\n",
    "    \n",
    "    # When each pdf has been exported to image files, display the following:\n",
    "    print(pdf_name + \" successfully exported.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results in the [pdf2image_example_jpgs](pdf2image_example_jpgs) example folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which files?\n",
    "\n",
    "For this lesson, we've given you only a sample of pages to work with from the 1955 NC session laws, beginning with the first chapter (page 1). You may have noticed that the file name ends with `0057` rather than `0001`. This is because scanned session law volumes often include other content, such as a scan of the outer covers, a table of contents, and the North Carolina state constitution, appearing before or after the laws themselves. Because the *On The Books Team* is only interested in the laws themselves, we went through each volume and removed all images that we did't need. The lesson here: **don't run OCR on content that you don't need.** \n",
    "\n",
    "<img src=\"images/08-ocr-03.jpeg\" width=\"70%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screenshot of the filenames for the 1955 volume, showing that the first page of laws is the file ending in 0057.\" title=\"Screenshot of the filenames for the 1955 volume, showing that the first page of laws is the file ending in 0057.\" />\n",
    "\n",
    "When you are preparing a corpus for OCR, in addition to becoming familiar with potential issues you see on each page, remove all files that don't need to be OCR'ed. *If you want to keep a copy of the complete un-OCR'ed corpus, make a copy of it **before** deleting image files from the version that you will use to perform OCR.* If you find that there is a consistent number of pages that you want to remove at the beginning and/or end of each corpus, it's possible to use Python to do this. Use caution if you choose this route, though, as it can be hard to undo deletions performed programmatically. At the end of the day *you probably know better than your computer* which files you need and which you don't.\n",
    "\n",
    "#### Missing pages? Duplicate pages?\n",
    "\n",
    "Something else to note about the 1955 laws in particular: The first two files in the scanned volume, ending `0000` and `0001`, happen to be scans of pages in the middle of the volume. These could be included at the beginning for a few reasons: \n",
    "- the archivists performing the scan accidentally skipped these pages during the initial scan; or\n",
    "- the initial scan of these pages was in some way inadequate (perhaps the page was blurred, or the scanner malfunctioned).\n",
    "\n",
    "<img src=\"images/08-ocr-04.jpeg\" width=\"70%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screenshot of first files in the 1955 volume, which are pages scanned from the middle of the volume.\" title=\"Screenshot of first files in the 1955 volume, which are pages scanned from the middle of the volume.\" />\n",
    "\n",
    "Regardless of the reason, now is the right moment to **check whether these pages were indeed skipped** and make a note of this so that you can be sure they are included in the OCR and final dataset. As it happens, these pages were *not* skipped but were duplicates, so we would remove them. If you *don't* find pages like these out of order in your own corpus, either indicated by page numbers in the images themselves or by numbered file names skipping an integer, *it may not be worth your time to check every page* (particularly if you are dealing with thousands of pages). It's possible to identify skipped pages in the data structuring process and add them to the larger dataset. \n",
    "\n",
    "#### A Note on Cropping\n",
    "\n",
    "Our duplicate pages do also point out something else important: **whether you need to crop scanned pages.**\n",
    "\n",
    "<img src=\"images/08-ocr-05.jpeg\" width=\"70%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screenshot of the duplicate images of page 592, the left image not cropped and the right image cropped.\" title=\"Screenshot of the duplicate images of page 592, the left image not cropped and the right image cropped.\" />\n",
    "\n",
    "When documents are scanned, often there is more included in the image than just the document itself: the stand or supports for the document, color calibration targets, rulers, and anything else in close proximity to the document.  Archivists preparing scanned materials for the Internet Archive and other digital repositories may crop out all parts of a scanned image that are *not* part of the document, aiming to create image files of a relatively uniform size.\n",
    "\n",
    "If your images have not been cropped already, **here are a few resources for learning how to batch crop images:**\n",
    "- In Python: [this Jupyter Notebook explains how to prepare to crop](https://github.com/UNC-Libraries-data/OnTheBooks/blob/master/examples/marginalia_determination/marginalia_determination.ipynb), and [this Notebook implements the crop along with other adjustments we'll explore further here](https://github.com/UNC-Libraries-data/OnTheBooks/blob/master/examples/adjustment_recommendation/adjRec.ipynb)\n",
    "- [In Photoshop](https://helpx.adobe.com/photoshop/using/crop-straighten-photos.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Overview <a class=\"anchor\" id=\"pre-processing\"></a>\n",
    "This phase is all about testing to figure out the best adjustments and OCR settings for your corpus:\n",
    "\n",
    "1. **Create a folder of sample text from your corpus.** The size of the sample may depend on the corpus' size and homogeneity or heterogeneity, but it should be an amount that you and/or your team could review manually in a reasonably short period of time.\n",
    "2. **Look for potential issues & needed adjustments.** Issues may include skewed or rotated text, fade text, smudges or damage to the page, etc.\n",
    "2. **Run OCR on your sample.**\n",
    "3. **Review the output** to identify errors, **looking especially for error *patterns*** that could be addressed at a corpus level. ***Consider:*** Is OCR still the right method for your project?\n",
    "4. **Create a list of errors and possible adjustments** that you might use to address the errors. **Order the list based on which errors should be solved first--which might address the largest number of errors.** For example, it would be more important to fix rotated or skewed pages across the sample/corpus before trying to use erosion or dilation to make specific pages more legible to Tesseract. \n",
    "5. **Make the first adjustment** on your list to the sample.\n",
    "6. **Re-run OCR on your sample.**\n",
    "7. **Review the output.** Has the output improved noticeably? Are there still errors and error patterns? \n",
    "8. **Repeat some or all of the above steps:** Depending on your findings, you might continue applying adjustments from your list, re-running OCR, and reviewing outputs, or you might be ready to move on to the [next step](#performing). \n",
    "\n",
    "Depending on the *complexity* of your corpus, you may want to select a few different samples to complete this process with and then compare the adjustments you make across all samples to see which adjustments work best overall, and which might *introduce* errors to certain parts of the corpus. \n",
    "\n",
    "If need be, you could consider running OCR on separate parts of your corpus. The *On The Books* team did this because marginalia (text printed in the margins) appear through a significant portion of the corpus but are no longer printed after a certain point. Removing marginalia wasn't necessary once it stopped appearing in the text, so that step could be skipped for the later portion of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Step-by-Step\n",
    "\n",
    "#### 1. Create a folder of sample text from your corpus. \n",
    "\n",
    "This has been done [here for this lesson](sample). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Look for potential issues & needed adjustments.\n",
    "\n",
    "Now let's take a closer look at the text we'll use to practice performing OCR:\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <p>Drawing from <a href=\"https://onthebooks.lib.unc.edu/laws/all-laws/\" alt=\"On the Books corpus\"><em>On The Books</em> corpus</a>, we'll be working with the North Carolina session laws from 1955: <a href=\"https://archive.org/details/sessionlawsresol1955nort/\" alt=\"1955 NC session laws on the Internet Archive\">https://archive.org/details/sessionlawsresol1955nort/</a>.</p>\n",
    "    <p>Open the 1995 volume in the Internet Archive and skim through, considering the following questions:</p>\n",
    "    <ul>\n",
    "        <li>How is the text formatted on the page?</li>\n",
    "        <li>How is the text oriented on the page?</li>\n",
    "        <li>Do you notice any pages or sections that might cause an error in the OCR? What are they? What kind of error do you think they might cause?</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "There are a few possible considerations with this volume that we'll want to address with pre-processing:\n",
    "- broken, or hyphenated, words\n",
    "- possible noise (e.g. the visible shadow of text from other pages)\n",
    "- slanted, or skewed, text\n",
    "- crooked, or rotated, text\n",
    "- occasional pencil marks on the text\n",
    "\n",
    "Did you notice any other possible issues? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A deeper dive into the issues:\n",
    "\n",
    "While the above looks promising--and we could go ahead and create a loop to run this code on every image file in the 1955 volume--we always run the risk of introducing errors into our plain text output that can be avoided by some pre-processing steps. So let's pause on the OCR for a bit and look at some of the steps you might need to take into consideration with your own materials. Although Tesseract does an overall good job addressing these issues when they are minor, it may be worth your while to fix any issues you notice *before* running Tesseract to avoid introducing errors into OCR'ed text in the first place:\n",
    "\n",
    "#### Rescaling\n",
    "**The higher quality the digitization, the better the OCR**--this is the general rule. \"Quality\" has a lot to do with the OCR requirements we've already covered as well as those we'll cover below. We can begin, though, with the number of pixels per image--that is, the number of pixels per *inch*. Remember that computers present images as a grid of pixels, usually squares but sometimes rectangles, and that each carry specific color information. Put hundreds, thousands, millions of pixels together, and we have an image. \n",
    "\n",
    "<img src=\"images/07-ocr-01.jpeg\" width=\"70%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screenshot of text stored in an image format from a page of North Carolina laws\" title=\"Screenshot of text stored in an image format from a page of North Carolina laws\" />\n",
    "\n",
    "A common way for computer programmers to measure image quality is by assessing the number of pixels per inch (ppi). This is important for many reasons: a photographer will want to keep their number of pixels high (perhaps 300 ppi) in preparation for printing, but a web designer will want a much lower number of pixels (72 ppi) to keep an image looking crisp while also keeping file sizes small to avoid slowing down webpage loading time. If you've ever opened a webpage and seen text but had to wait a few seconds for images to load, you've seen the difference between how long it takes for text vs. an image to load. The more pixels, the larger the file (in kilobytes, megabytes, or even gigabytes), and large files take longer to move from a server to your computer--add in low bandwidth internet, and the load time increases exponentially. \n",
    "\n",
    "So, what's the difference? Let's look:\n",
    "\n",
    "<div class=\"row\" style=\"padding-bottom:20px;\">\n",
    "    <div class=\"column\">\n",
    "<img src=\"images/08-ocr-06.jpeg\" width=\"40%\" style=\"float:left; padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"An image of the letter S at 72 ppi.\" title=\"An image of the letter S at 72 ppi.\" />\n",
    "    </div>\n",
    "    <div class=\"column\">  \n",
    "<img src=\"images/08-ocr-07.jpeg\" width=\"39%\" style=\"float:right; padding-top:20px; margin-right:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"An image of the letter S at 300 ppi.\" title=\"An image of the letter S at 300 ppi.\" />\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "The image on the left shows a scanned letter S at 72 ppi. The visible squares represent individual pixels. Note that each pixel represents one color from the page, and there is a transition between pixels representing ink and those representing paper. \n",
    "\n",
    "The image on the right is the same letter S rescaled to 300 ppi. The squares here appear smaller because there are far more of them. Note that instead of there being only a line 1-2 pixels wide making up the S shape, there are far more--far more for Tesseract to \"read\" and interpret.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <p><a href=\"https://tesseract-ocr.github.io/tessdoc/ImproveQuality\" target=\"blank\">Per its documentation</a>, Tesseract works best with an image resolution of 300 ppi. The documentation actually uses \"dpi\", or <a href=\"https://en.wikipedia.org/wiki/Dots_per_inch\" target=\"blank\">\"dots per inch\"</a>. If you're beginning your project by scanning materials, this unit will be important when you set up your scanner, but once you move into image processing, we're dealing with <a href=\"https://en.wikipedia.org/wiki/Pixel_density\" target=\"blank\">pixels per inch</a>. These are not the same, but many people use dpi and ppi interchangeably.</p>\n",
    "</div>\n",
    "\n",
    "The images we downloaded from the Internet Archive are 72ppi--optimized for web viewing. In an ideal world, we'd go back and use the .jp2 files, which are higher resolution. While it's possible to rescale (increase the ppi) of our sample images, increasing ppi involves using algorithms that can change, in minute ways, an image file. Those minute changes could make a larger impact on OCR results depending on the original image. For this reason, **it's always better to start by creating high resolution image files in the scanning process.** For our purposes here, PyTesseract has already proven that it's working well with the .jpg 72 ppi versions of the Internet Archive files. If you're interested in learning more about rescaling images, though, try this [GeeksForGeeks tutorial](https://www.geeksforgeeks.org/python-pil-image-resize-method/) or these [Photoshop instructions](https://helpx.adobe.com/photoshop/using/resizing-image.html) as a starting point.\n",
    "\n",
    "We can also change the dpi using the following method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open an image.\n",
    "img = Image.open(\"images/07-ocr-02.jpeg\")\n",
    "\n",
    "img300 = img.save(\"images/07-ocr-02_300.jpeg\", dpi=(300, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's [see the result](images/07-ocr-02_300.jpeg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotating & Deskewing\n",
    "Sometimes, in spite of everyone's best efforts, a document is scanned at a slight angle, either rotated slightly on the scan bed or perhaps photographed at a slight angle, introducing a skew. The result can look like these images:\n",
    "\n",
    "<div class=\"row\"  style=\"padding-bottom:20px;\">\n",
    "    <div class=\"column\">\n",
    "<img src=\"images/sessionlawsresol1955nort_0057_rotated.jpg\" width=\"40%\" style=\"float:left; padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"A test OCR page that has been rotated; the first page of the 1955 North Carolina session laws.\" title=\"A test OCR page that has been rotated; the first page of the 1955 North Carolina session laws.\" />\n",
    "    </div>\n",
    "    <div class=\"column\">  \n",
    "<img src=\"images/sessionlawsresol1955nort_0057_skewed.jpg\" width=\"37%\" style=\"padding-left:10px; padding-top:20px; float:right; margin-right: 40px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"A test OCR page that has been skewed; the first page of the 1955 North Carolina session laws.\" title=\"A test OCR page that has been skewed; the first page of the 1955 North Carolina session laws.\" />\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "**Left:** A rotated version of our sample page. **Right:** A skewed version of our sample page.\n",
    "\n",
    "Let's run these through PyTesseract to see how it handles them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OCR the rotated page.\n",
    "print(pytesseract.image_to_string(Image.open('images/sessionlawsresol1955nort_0057_rotated.jpg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How do the OCR results for the rotated page compare to our attempt with the original (unrotated) file?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the skewed version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OCR the skewed page.\n",
    "print(pytesseract.image_to_string(Image.open('images/sessionlawsresol1955nort_0057_skewed.jpg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at this output carefully. The skewed page might appear to be largely similar to the original (non-skewed) page we tried first. Where are there errors, though? What kinds of problems might these errors cause for later analysis?\n",
    "\n",
    "*Why do errors occur when reading a rotated or skewed text?* Tesseract has been programmed to expect to \"read\" a language in the same way a human would. We read English left to right and from the top of a page down. Although we are able to parse text even when viewing it at an angle (maybe you can even read text upside down), Tesseract doesn't do this well. It will still attempt to read a rotated line from left to right--it won't know to follow the text as it slants down or up. So it returns its interpretation of the letters that fall within its line of \"sight.\" This is why, particularly with rotated texts, we may receive symbols and other unexpected characters.\n",
    "\n",
    "*It's possible to [straighten scanned images using Python](https://becominghuman.ai/how-to-automatically-deskew-straighten-a-text-image-using-opencv-a0c30aed83df), but this is beyond the scope of our course.*\n",
    "\n",
    "#### Removing Noise\n",
    "Images can't produce sound, but they can still have *noise*. In an image, noise is a **random variation in brightness or color**. Let's look again at our S from earlier:\n",
    "\n",
    "<img src=\"images/08-ocr-06.jpeg\" width=\"51%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"An image of the letter S at 72 ppi.\" title=\"An image of the letter S at 72 ppi.\" />\n",
    "\n",
    "The pixels surrounding the S represent the color of the paper the S was printed on. The pixels are not all one color, though. That variation is noise. In these images, the noise has already been minimized in the scanning process: if you open one of the images and zoom in, you may notice that blank page space surrounding text appears to have many pixels that are close to the same color. \n",
    "\n",
    "**Tesseract removes noise on its own, but this process can also introduce errors in images that have a high amount of noise.** If you want to learn more about noise and removing it using Python [here's a good place to start](https://docs.opencv.org/3.4/d5/d69/tutorial_py_non_local_means.html). \n",
    "\n",
    "\n",
    "#### Inverting & Binarizing\n",
    "Early OCR programs required light text on dark backgrounds to operate correctly. In recent years, many OCR programs have moved to preferring dark text on light backgrounds. This means that **inversion** is typically not an issue historians need to worry about since most printed documents are dark text on light background. There might be some exceptions to this if you are working with, for example, images of microfiche. \n",
    "\n",
    "Just for fun, let's see [how Python handles image inversion](https://pillow.readthedocs.io/en/latest/reference/ImageOps.html#PIL.ImageOps.invert) (and you can use this if you ever do need to OCR microfiche):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the original image file.\n",
    "file = Image.open(\"sessionlawsresol1955nort_0057.jpg\")\n",
    "\n",
    "# Use the ImageOps.invert function to invert the colors in the original file.\n",
    "inverted_file = ImageOps.invert(file)\n",
    "\n",
    "# Save the newly inverted image file.\n",
    "inverted_file.save(\"sessionlawsresol1955nort_0057_inverted.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll find the result saved in the [same folder as this Notebook](sessionlawsresol1955nort_0057_inverted.jpg), or you can preview it below:\n",
    "\n",
    "<img src=\"images/sessionlawsresol1955nort_0057_inverted.jpg\" width=\"40%\" style=\"padding-top:20px; margin-bottom:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Inverted version of page 57 from the 1955 North Carolina Session Laws.\" title=\"Inverted version of page 57 from the 1955 North Carolina Session Laws.\" />\n",
    "\n",
    "Where inversion switches the colors in an image file **binarization converts an image so that it shows image data in only two pixel \"colors\": black and white**. Tesseract does this as part of its OCR process, but it might be worthwhile to do this ahead of time if you're trying to reduce noise or see where, for example, a shadow on a page may introduce problems for Tesseract. A step in that direction that can help is converting images to grayscale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "# Open the original image file.\n",
    "file = Image.open(\"sessionlawsresol1955nort_0057.jpg\")\n",
    "\n",
    "# Use the Image.convert function to change the original file to black & white.\n",
    "# The mode \"1\" below will return a 1-bit per pixel image.\n",
    "# Change this to \"L\" to create an 8-bit per pixel image.\n",
    "binarized_file = file.convert(\"L\")\n",
    "\n",
    "# Save the new grayscale image file.\n",
    "binarized_file.save(\"sessionlawsresol1955nort_0057_binarized.jpg\")\n",
    "\n",
    "display(binarized_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result ([again saved with this Notebook](sessionlawsresol1955nort_0057_grayscale.jpg)) in our case is less exciting because our image already had dark ink on a light background. Nonetheless, it helps us see more clearly where there might be variations in background color that will interfere with OCR. In this case, a smudge in the top right corner, far from the text, has become much more visible, as has the shadow near the gutter (where the page meets the volume's binding). Gutter shadows can cause problems if there is little margin between the gutter and page text. Tesseract has an [example of a page](https://tesseract-ocr.github.io/tessdoc/ImproveQuality#binarisation) that demonstrates much more dramatically how binarization can reveal shadows on a page.\n",
    "\n",
    "<img src=\"images/sessionlawsresol1955nort_0057_grayscale.jpg\" width=\"40%\" style=\"padding-top:20px; margin-bottom:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Grayscale version of page 57 from the 1955 North Carolina Session Laws.\" title=\"Grayscale version of page 57 from the 1955 North Carolina Session Laws.\" />\n",
    "\n",
    "#### Dilation and erosion\n",
    "Finally, there is dilation and erosion. As we can see in our sample page, printers often varied font thickness when they set the text type. Bold might be used for headings while thinner fonts might be used for smaller text. Depending on the print quality, bolded text might have additional ink around it, while thinner text might not have enough ink. Variation in ink thickness can throw Tesseract off, so **eroding bolded text** (making it thinner) and **dilating very thin text** (adding thickness) can help address this issue.\n",
    "\n",
    "Performing erosion and dilation in Python requires some additional understanding of image processing. We won't cover it here (and our samples don't need it!), but [this GeeksForGeeks tutorial](https://www.geeksforgeeks.org/erosion-dilation-images-using-opencv-python/) explains the basics and provides sample code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing OCR <a class=\"anchor\" id=\"performing\"></a>\n",
    "Once you're satisfied with the pre-processing on your sample(s), here's where you run the actual OCR. This part of the process may also be iterative:\n",
    "\n",
    "1. **Apply your chosen [pre-processing](#pre-processing) adjustments** to your entire corpus.\n",
    "2. **Run OCR** on your entire corpus.\n",
    "3. **Pull samples from your output to review.** Do you notice any recurring or new errors? If so, you may need to return to pre-processing to assess and address these errors.\n",
    "4. **Repeat steps 1-3 as needed.** If you have a very large corpus, you may consider running these steps in batches and iterating through each batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Code\n",
    "\n",
    "Let's review the code from [Lesson 01](1-WhatIsOCR.ipynb).\n",
    "\n",
    "First, we need to check that tesseract is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tesseract on Binder.\n",
    "# The exclamation runs the command as a terminal command.\n",
    "# This may take 1-2 minutes.\n",
    "# Source: Nathan Kelber & JStor Labs Constellate team.\n",
    "!conda install -c conda-forge -y tesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following let's us OCR 1 image and display the output in Jupyter Notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Image module from the Pillow Library, \n",
    "# which will help us access the image.\n",
    "from PIL import Image\n",
    "\n",
    "# Import the pytesseract library, which will run the OCR process.\n",
    "import pytesseract\n",
    "\n",
    "# OPTIONAL: Specify OEM & PSM configurations. \n",
    "# 3 is the default setting for both.\n",
    "custom_oem_psm_config = r'--oem 3 --psm 3'\n",
    "\n",
    "# Open a specific image file, convert the text in \n",
    "# the image to computer-readable text (OCR),\n",
    "# and then print the results for us to see here.\n",
    "print(pytesseract.image_to_string(Image.open(\"sessionlawsresol1955nort_0057.jpg\"), lang=\"eng\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code lets us OCR 1 image and save the output as a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the image file. (Assign it to a variable.)\n",
    "# You can change the filename in quotes below to OCR a different file.\n",
    "file = \"sessionlawsresol1955nort_0057.jpg\"\n",
    "\n",
    "# Open the file named above. \n",
    "# While it's open, do several things:\n",
    "with open(file, 'rb') as inputFile:\n",
    "        \n",
    "    # Read the file using PIL's Image module.\n",
    "    img = Image.open(inputFile)\n",
    "\n",
    "    # Run OCR on the open file.\n",
    "    ocrText = pytesseract.image_to_string(img, lang=\"eng\")\n",
    "        \n",
    "    # Get a file name--without the extension-- \n",
    "    # to use when we name the output file.\n",
    "    fileName = file.strip('.jpg')\n",
    "\n",
    "# The image file above will be closed before moving on to this line.\n",
    "# The OCR'ed text has been pulled from the image and stored in\n",
    "# a Python variable for us to continue to use.\n",
    "\n",
    "# Create and open a new text file, name it to match its input file,\n",
    "# declare its encoding to be UTF-8 so that it correctly outputs\n",
    "# non-ASCII characters,\n",
    "with open(fileName + \".txt\", \"w\", encoding=\"utf-8\") as outFile:\n",
    "        \n",
    "    # and write the OCR'ed text to the file.\n",
    "    outFile.write(ocrText)\n",
    "\n",
    "# Display a message to let us know the file has been created\n",
    "# and the script successfully completed.\n",
    "print(fileName, \"text file successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we're planning to OCR a bunch of pages. Can we use 1 script to do that automatically? YES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os, a module for file management.\n",
    "import os\n",
    "\n",
    "# Import re, a module that we can use to search text.\n",
    "import re\n",
    "\n",
    "# Import glob, a module that helps with file management.\n",
    "import glob\n",
    "\n",
    "# Open the file folder where our sample pages are stored.\n",
    "# Look only for the files ending with the \".jpg\" file extension.\n",
    "sampleFilePath = glob.glob(\"sample/*.jpg\")\n",
    "\n",
    "# Create a folder for the volume in the output directory (/sample).\n",
    "outDir = \"sample_output\"\n",
    "newDir = os.path.normpath(outDir)\n",
    "\n",
    "# If you're running this script a second or third time, the sample_output folder will \n",
    "# already exist. \n",
    "# The following statement checks whether it already exists and then creates the\n",
    "# sample_output folder if it doesn't exist (e.g. if the statement below is False).\n",
    "if os.path.exists(newDir) == False:\n",
    "    os.mkdir(newDir)\n",
    "\n",
    "# Adding a \"/\" after newDir (\"sample_output\") makes it into a file path that\n",
    "# we'll use to move our output file to the correct folder later in this script.\n",
    "newDir = newDir + \"/\"\n",
    "    \n",
    "# For each file in the sample folder:\n",
    "for file in sampleFilePath:\n",
    "    \n",
    "    # Open a file.\n",
    "    with open(file, 'rb') as inputFile:\n",
    "        \n",
    "        # Read the file using PIL's Image module.\n",
    "        img = Image.open(inputFile)\n",
    "    \n",
    "        # Run OCR on the open file.\n",
    "        ocrText = pytesseract.image_to_string(img, lang=\"eng\")\n",
    "        \n",
    "        # Get a file name -- without the extension -- to use when we name the output file.\n",
    "        fileName = file.strip('.jpg')\n",
    "        \n",
    "        # The current file name also includes its folder name (sampleFilePath, \"sample/\").\n",
    "        # We want to store our text output files in a different folder so that we can use \n",
    "        # them in future without altering the original image files. The following two \n",
    "        # lines use the re module to rename the path from \"sample/\" to \"sample_output/\",\n",
    "        # which also changes the final destination for our next text file.\n",
    "        currentFolder = \"sample/\"\n",
    "        fileName = re.sub(currentFolder, newDir, fileName)\n",
    "\n",
    "        # Create and open a text file, name it to match its input file,\n",
    "        # and write the OCR'ed text to the file.\n",
    "        with open(fileName + \".txt\", \"w\") as outFile:\n",
    "            outFile.write(ocrText)\n",
    "        \n",
    "        print(fileName, \" successfully created.\")\n",
    "    \n",
    "    # Loop back to check for another image file, run OCR on that file, \n",
    "    # and write its OCR to a new output file. When no more files remain,\n",
    "    # this loop will end, and the script will be finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our [output](sample_output).\n",
    "\n",
    "*If we wanted to, we could rework this script to iterate over multiple folders...how might we combine some of the code from the earlier pdf2image process with this script to do that?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Cleaning\" OCR (Post-Processing) Overview <a class=\"anchor\" id=\"post-processing\"></a>\n",
    "\n",
    "**This part of the process is often best performed with a combination of manual (human) and automated (computer) steps.** This is where you may be addressing not only errors in the OCR itself but also issues with the original printing, as we describe below with regard to [hyphenated words at the end of lines](#hyphens). As with pre-processing, how complex you make iterations in this phase depends on your corpus and your resources:\n",
    "\n",
    "1. **Review the OCR output.** Take an initial look at the OCR text files. Sometimes even just a glance will give you a sense of how well the process has gone. If you see a lot of errors, return to the [pre-processing](#pre-processing) questions and consider which steps you might take to improve the OCR output.\n",
    "\n",
    "\n",
    "2. **Run a spellchecker & calculate the quality of the OCR output.** Use a spellchecker to get a sense of just how accurate the OCR process may have been. Note that spellchecking here, as with spellchecking in software such as Word, is really looking for known and unknown words.\n",
    "\n",
    "\n",
    "3. **Use Python to check for and correct possible recurring & unique spelling errors.** These are errors that appear frequently and may be caused by the typescript, hyphenation at the end of lines, or other patterns that Tesseract repeatedly misinterprets. This step should focus on common words and avoid proper nouns (unless you have a full list of proper nouns to draw from). As with any automated step, it's possible that new errors will be introduced here. If there is a known and small quantity of proper nouns used in individual texts or across the corpus, and these are consistently \"read\" incorrectly by Tesseract, it may be possible to use Python to correct these.\n",
    "\n",
    "\n",
    "4. If your corpus is small enough and/or you have a team that can help you, **read through the corpus to manually check for and correct unique errors**. This may be a moment to correct proper nouns. If you have a team, it may be advisable to have texts read and corrected by multiple team members. It will be important that these team members have access to both inputs and outputs, and perhaps even lists of proper nouns, to be able to compare the original scans with the computer-readable versions. You may even want to set up a process whereby reviewers can flag words they are not sure about so that another reviewer can provide their opinion so that you and/or another project manager making a final decision on uncertain words.\n",
    "\n",
    "\n",
    "The above process could be broken down further to address smaller issues incrementally and iteratively. It may also be useful to break your corpus into units of analysis before or during this process to assist with cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Processing Step-by-Step\n",
    "\n",
    "#### 1. Review the OCR output.\n",
    "\n",
    "Locate the [\"sample_output\" folder](sample_output) accompanying this tutorial, and take a look at the text files it should now contain. (Note that we have included these files with the tutorial in case you run into trouble running the script above.) Compare them to the [.jpg image files in the \"sample\" folder](sample). What do you notice?\n",
    "\n",
    "Now that we've looked at all of the files, look more closely at `sessionlawsresol1955nort_0057.jpg` and `sessionlawsresol1955nort_0057.txt`.\n",
    "\n",
    "#### 2. Check for misspellings & quality. <a class=\"anchor\" id=\"hyphens\"></a>\n",
    "\n",
    "Although it appears that this page has been entirely correctly OCR'ed, there are two issues that show up in this text file that we want to address in all of our OCR'ed files:\n",
    "\n",
    "1. The original printers **broke words at the end of some lines**. For example, `Dis-trict` and, at the very end of the page, `twenty-`. How do we deal with this without removing words that *should* be hyphenated?\n",
    "2. **How would we know how accurate this simple script might be when applied to the entire volume, or to the entire corpus?** \n",
    "\n",
    "In addition to being hyphenated, `Dis-trict` may be misspelled as `Dis-triet` or `Dis-trism` in our output -- is this just one instance, or does this error recur? If it's recurring, we can use Python to fix it across the corpus. This could be more efficient than having to read the entire OCR'ed corpus. A good starting point is to get a sense of just how accurate the OCR process has been, that is **check its readability**, before we start trying to identify and fix spelling errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the following scripts, we'll look at how to correct misspelling and check for OCR accuracy by generating a readability score.** During this process, we'll remove the hyphens at the end of lines to help us with spellchecking, but we may find that we introduce new issues for the spellcheck.\n",
    "\n",
    "To begin, there are a number of modules and libraries we need to import (or reimport) to extend Python's functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTesseract and PIL, an image processing library used by PyTesseract, to complete the OCR.\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "# Import os, a module for file management.\n",
    "import os\n",
    "\n",
    "# Import re, a module that we can use to search text.\n",
    "import re\n",
    "\n",
    "# Import glob, a module that helps with file management.\n",
    "import glob\n",
    "\n",
    "# Import the SpellChecker module, which we'll use to look for likely misspelled words.\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Import the word_tokenize module from the nltk (\"Natural Language Processing Kit\") library.\n",
    "# NLTK is a powerful toolset we can use to manipulate and analyze text data.\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# We'll also need the pandas library, which is a powerful toolset for managing data.\n",
    "# We'll learn more about pandas in the exploratory analysis modules.\n",
    "import pandas as pandas\n",
    "\n",
    "# This statement confirms that the above code was run without issue.\n",
    "print(\"Modules & libraries imported. Ready for the next step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll set up variables that we'll use to give Python information and structure information that Python returns. These include the location of the original image files and the place we want to store our OCR'ed text, as well as a [spellcheck dictionary](https://pypi.org/project/pyspellchecker/), which we'll extend to include [North Carolina placenames](geonames.txt), and a dataframe (essentially, an empty table) we'll use to structure readability information along with the OCR'ed text.\n",
    "\n",
    "*Note: The [spellchecker library](https://pypi.org/project/pyspellchecker/) we are using support a limited number of Western languages. English is the default.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file folder where our sample pages are stored.\n",
    "# Look only for files ending with the \".jpg\" file extension.\n",
    "sampleFilePath = glob.glob(\"sample/*.jpg\")\n",
    "\n",
    "# Before we loop through each page, we'll augment our spellchecker \n",
    "# dictionary to include place names specific to North Carolina. \n",
    "# Our script for gathering these place names is available here: \n",
    "# https://github.com/UNC-Libraries-data/OnTheBooks/blob/master/examples/adjustment_recommendation/geonames.py\n",
    "\n",
    "# Load the spellchecker dictionary.\n",
    "# Replace the language attribute with another 2 letter code\n",
    "# to select another language. Options are: English - ‘en’, Spanish - ‘es’,\n",
    "# French - ‘fr’, Portuguese - ‘pt’, German - ‘de’, Russian - ‘ru’.\n",
    "\n",
    "spell = SpellChecker(language='en')\n",
    "\n",
    "# Add the place name words from the \"geonames.txt\" file to the \n",
    "# spellchecker dictionary.\n",
    "spell.word_frequency.load_text_file(\"geonames.txt\")\n",
    "\n",
    "# We'll use Pandas to create a dataframe (a table) that can hold \n",
    "# information about an OCR'ed page and display it in a tabular format.\n",
    "# This dataframe will start out empty with only its column headers \n",
    "# defined. We'll add information to it one page at a time. So each\n",
    "# row will represent 1 page.\n",
    "df = pandas.DataFrame(columns=[\"file_name\",\"token_count\",\"unknown_count\",\"readability\",\"unknown_words\",\"text\"])\n",
    "\n",
    "# This statement confirms that the above code was run without issue.\n",
    "print(\"Variables created. Ready for the next step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what each column will hold:\n",
    "\n",
    "- **file_name**: The name for the corresponding image file. For now, this is the only information in the table that identifies where the rest of the information in each row comes from (which page).\n",
    "- **token_count**: The total number of tokens (words) found in each page.\n",
    "- **unknown_count**: The number of unknown (\"misspelled\") words found in each page.\n",
    "- **readability**: Think of this as the percentage of the page that was readable.\n",
    "- **unknown_words**: A list of tokens (words or in some cases characters) that were not listed in the spellchecker.\n",
    "- **text**: The OCR'ed text output from each page. The output here includes all <a href=\"https://en.wikipedia.org/wiki/Escape_character#JavaScript\" target=\"blank\">escape characters</a>, so it may look as if a lot of erronenous characters have been added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll remove hyphens from the text, run the spellcheck script, and produce a dataframe (table) of information that will give us a sense of the accuracy of our OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each file in the sample folder:\n",
    "for file in sampleFilePath:\n",
    "    \n",
    "    # Open a file.\n",
    "    with open(file, 'rb') as inputFile:\n",
    "        \n",
    "        # Get a file name--without the extension-- \n",
    "        # to use when we name the output file.\n",
    "        fileName = os.path.split(file)[1]\n",
    "        \n",
    "        # Read the file using PIL's Image module.\n",
    "        img = Image.open(inputFile)\n",
    "    \n",
    "        # Run OCR on the open file.\n",
    "        ocrText = pytesseract.image_to_string(img, lang=\"eng\")\n",
    "        \n",
    "        # Join hyphenated words that are split between lines by \n",
    "        # looking for a hyphen followed by a newline character: \"-\\n\"\n",
    "            # \"\\n\" is an \"escape character\" and represents the \n",
    "            # \"newline,\" a character that is usually invisible \n",
    "            # to human readers but that computers use to mark the \n",
    "            # end/beginning of a line. Each time you press the \n",
    "            # Enter/Return key on your keyboard, an invisible \"\\n\" \n",
    "            # is created to mark the beginning of a new line.\n",
    "        ocrText = ocrText.replace(\"-\\n\",\"\")\n",
    "        \n",
    "        # Now we'll check spellings and insert corrections!\n",
    "        \n",
    "        # First, we'll use NLTK to \"tokenize\" text. \n",
    "            # \"Tokenize\" here means to take a page of our OCR'ed text,\n",
    "            # which Python is currently reading as one big glob of data,\n",
    "            # and separate each word out so that it can be read as an\n",
    "            # individual piece of data within a larger data structure \n",
    "            # (a list). This process also removes punctuation.\n",
    "        tokens = word_tokenize(ocrText)\n",
    "        \n",
    "        # Next, we'll convert all of those tokens (words) into \n",
    "        # lowercase because the spellcheck dictionary is in all \n",
    "        # lowercase, and the spellcheck process is case sensitive.\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "        \n",
    "        # We'll make sure that our text data complies with a universal \n",
    "        # text format so that all characters in the data and the \n",
    "        # spellchecker can be matched.\n",
    "        tokens = [token.encode(\"utf-8\", errors = \"replace\") for token in tokens]\n",
    "        \n",
    "        # Now we can get all of the words that don't match the \n",
    "        # spellchecker dictionary or our list of place names--\n",
    "        # these are the potential spelling errors.\n",
    "        unknown = spell.unknown(tokens)\n",
    "        \n",
    "        # Let's use a little math to find out how many potential \n",
    "        # spelling errors were identified. As part of this process, \n",
    "        # we'll create a \"readability\" score that will give us a \n",
    "        # percentage of how readable each file is--how much of the \n",
    "        # OCR'ed is \"correct.\"\n",
    "        \n",
    "        # If the list of unknown tokens (words) is greater than 0 \n",
    "        # (i.e. if the list is not empty):\n",
    "        if len(unknown) != 0:\n",
    "            \n",
    "               # Following order of operations, here's what's happening \n",
    "               # in the readability variable below:\n",
    "               # 1. Divide the number of unknown tokens (len(unknown)) \n",
    "                    # by the total number of tokens on the page\n",
    "                    # (len(tokens)). Use \"float\" to specify that Python\n",
    "                    # returns a decimal number:\n",
    "                        # (float(len(unknown))/float(len(tokens))\n",
    "               # 2. Multiply the number from step 1 by 100.\n",
    "                    # (float(len(unknown))/float(len(tokens)) * 100)\n",
    "               # 3. Subtract the number from step 2 from 100.\n",
    "                    # 100 - (float(len(unknown))/float(len(tokens)) * 100)\n",
    "               # 4. Round the number from step 3 to 2 decimal places\n",
    "                    # round(100 - (float(len(unknown))/float(len(tokens)) * 100), 2)\n",
    "            \n",
    "           readability = round(100 - (float(len(unknown))/float(len(tokens)) * 100), 2)\n",
    "        \n",
    "        # If the list of unknown tokens is empty (or equal to 0), then readability is 100!\n",
    "        else:\n",
    "           readability = 100\n",
    "    \n",
    "        # Let's create a record of the readability information \n",
    "        # for this page that we'll add to the dataframe. \n",
    "        # The following is a Python dictionary, another way of \n",
    "        # storing data. Each word or phrase to the left of the : is a\n",
    "        # \"key\" -- think of it as a column header. Each piece of \n",
    "        # information to the right is a \"value\" -- information \n",
    "        # written in a single cell below each header. \n",
    "        # Altogether, this dictionary represents 1 row (\"imgRecord\") \n",
    "        # in a table (or dataframe).\n",
    "        imgRecord = {\n",
    "                \"file_name\" : fileName,\n",
    "                \"token_count\" : len(tokens),\n",
    "                \"unknown_count\" : len(unknown),\n",
    "                \"readability\" : readability,\n",
    "                \"unknown_words\" : list(unknown),\n",
    "                \"text\" : ocrText\n",
    "                }\n",
    "        \n",
    "        # Here's where we'll add all the information we gathered in \n",
    "        # imgRecord as a row in our dataframe.\n",
    "        df = df.append(imgRecord, ignore_index=True)\n",
    "\n",
    "        \n",
    "        # This statement lets us know if a page has been successfully \n",
    "        # checked for readability.\n",
    "        print(fileName, \"checked for readability.\")\n",
    "    \n",
    "# This time, instead of creating individual .txt files for each page,\n",
    "# we're going to save all of the OCR'ed text and readability \n",
    "# information to a single .csv (\"comma separated value\") file. \n",
    "# We can view this file format as a table. Having everything stored \n",
    "# like this will help us with clean up and future analysis.\n",
    "df.to_csv(r'sample_output/sample_output_spellchecked.csv', header=True, index=True, sep=',')\n",
    "\n",
    "# We have the data stored in a file now, but we can also \n",
    "# preview it here:\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also open [sample_output_spellchecked.csv](sample_output/sample_output_spellchecked.csv) to view the full dataset. (Open in a text editor if possible.) You'll find it in the sample_output folder.\n",
    "\n",
    "**Let's consider what these columns can tell us:**\n",
    "- The number of uknown words in each page is low (3 max.), and the readability score for each is near 100. This means that *on each page there are only a few errors that need to be addressed.*\n",
    "- The list of unknown words shows that there are some errors that repeat on all or most pages. These include `ch`, whic is likely the abbreviation for `chapter` that occurs frequently throughout the text. These \"errors\" can be ignored.\n",
    "- What else do the columns tell you?\n",
    "\n",
    "\n",
    "#### 3. & 4. Correcting Errors\n",
    "\n",
    "Broadly speaking, we can break down errors into two categories: **unique** or **recurring**. We can use Python to address both types to an extent, but it's likely that some manual review will still need to be done to ensure the highest quality OCR. Whether and how much manual review can be done will depend on the project's resources.\n",
    "\n",
    "##### Unique Errors\n",
    "\n",
    "There are at least **two ways to address unique computer-identified errors:**\n",
    "\n",
    "1. Since we produced a list of unknown words in our readability test, we could simply open each file in a text editor and use find-and-replace functionalities (Command + F or Control + F) to locate and replace instances of unique errors.\n",
    "\n",
    "\n",
    "\n",
    "2. We could use a little Python to find and replace these errors across the corpus. \n",
    "\n",
    "*Caveat: There may be instances where variant spellings are identified as \"unknown\" (misspelled) but are true representations of the word as it was originally printed. It may be necessary to check these misspellings against the scanned pages and decide whether or not to correct the text in the OCR output.*\n",
    "\n",
    "The following script runs through the entire sample output (and could be applied to an entire corpus) and checks for and replaces instances of a unique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import glob, a module that helps with file management.\n",
    "import glob\n",
    "\n",
    "# Identify the sample_output file path.\n",
    "# Remember that our readability output is also stored \n",
    "# in this file as a .csv. We don't want to change it, \n",
    "# so we'll use glob to look for only .txt files.\n",
    "filePath = glob.glob(\"sample_output/*.txt\")\n",
    "\n",
    "# Apply the following loop to one file at a time in filePath.\n",
    "for file in filePath:\n",
    "    \n",
    "    # Open a file in \"read\" (r) mode.\n",
    "    text = open(file, \"r\")\n",
    "    \n",
    "    # Read in the contents of that file.\n",
    "    text = text.read()\n",
    "    \n",
    "    # Find instances of and unknown word and replace\n",
    "    # with a known word.\n",
    "    \n",
    "    unknown_word = \"commissiuner\"\n",
    "    \n",
    "    known_word = \"commissioner\"\n",
    "    \n",
    "    word_correction = text.replace(unknown_word, known_word)\n",
    "    \n",
    "    # Close the file.\n",
    "    # file.close()\n",
    "    \n",
    "    # Reopen the file in \"write\" (w) mode.\n",
    "    file = open(file, \"w\")\n",
    "    \n",
    "    # Add the changed word into the reopened file.\n",
    "    file.write(word_correction)\n",
    "    \n",
    "    # Close the file.\n",
    "    file.close()\n",
    "\n",
    "print(\"All instances of \" + unknown_word + \" replaced with \" + known_word + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the [sample_output files](sample_output) for the unknown word to see if the word is still present. \n",
    "\n",
    "We've done this for one word at a time, but we could use the list of unknown words generated to create a script that runs through the list and corrects each instance all at once--rather than running the above script for each correction individually.\n",
    "\n",
    "##### \"Errors\" that computers don't notice.\n",
    "\n",
    "There may be times when computers don't recognize errors that humans will notice (hence the need for manual review). When this happens, we can use Python to search for and correc those errors. Here's an example from [sessionlawsresol1955nort_0057](sessionlawsresol1955nort_0057.txt):\n",
    "\n",
    "`The General Assembly 0/ Alarm Carolina do amt:`\n",
    "\n",
    "In the above there are several errors, but let's focus on `Alarm`, which should be `North`. We can use the same script we used above to fix this error, but we want to avoid changing correct uses of the word \"alarm\" to \"north.\" *What can we do to ensure we only change usages such as the above?*\n",
    "\n",
    "##### Recurring Errors & Changes\n",
    "\n",
    "There are several kinds of recurring errors:\n",
    "\n",
    "- Specific Words & Phrases (if a unique mispelling above is present consistently across the corpus, for example).\n",
    "- Word, Phrase, or Character Patterns (for example, a hyphen used to break up a word at the end of a line).\n",
    "\n",
    "We looked earlier at how to remove hyphens at the end of lines. To do this we replaced `-\\n` with nothing (\"\"). We saved that change to spellcheck csv, but we could have written that to the original text files. We could use the above script to make that change directly in the original output files, though it may be advisable to *keep the original text output files separate from the corrected versions in case you need to refer back.*\n",
    "\n",
    "We could also use the below script in combination with [regular expressions](https://en.wikipedia.org/wiki/Regular_expression) to correct issues that we know are recurring. For example, a look at the [sample documents](sample) reveals that the following line occurs at the beginning of each chapter:\n",
    "\n",
    "`The General Assembly of North Carolina do enact:`\n",
    "\n",
    "The OCR has read this line in various ways. We could attempt to use regular expressions to identify these variations and change them all to match the above using some version of the following regular expression:\n",
    "\n",
    "`\\n\\nThe General Assembly.*?t:\\n\\n`\n",
    "\n",
    "**Be careful when attempting changes with regular expressions**--these always come with the risk of introducing new errors. To avoid as many as possible, make your regular expression as specific as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regular expressions module (re), \n",
    "# which helps us use regex in Python.\n",
    "import re\n",
    "\n",
    "# Import glob, a module that helps with file management.\n",
    "import glob\n",
    "\n",
    "# Identify the sample_output file path.\n",
    "# Remember that our readability output is also stored \n",
    "# in this file as a .csv. We don't want to change it, \n",
    "# so we'll use glob to look for only .txt files.\n",
    "filePath = glob.glob(\"sample_output/*.txt\")\n",
    "\n",
    "# Save the pattern for a chapter header (even pages) that we \n",
    "# want to search each page for. We've added \"^\" to our regular \n",
    "# expressions to be extra sure that Python searches only at the \n",
    "# beginning of each file.\n",
    "regex_search = re.compile(\"\\n\\nThe General Assembly.*?t:\\n\\n\")\n",
    "\n",
    "# Save the text that we want to use to correct the OCR output.\n",
    "replacement = \"\\n\\nThe General Assembly of North Carolina do enact:\\n\\n\"\n",
    "\n",
    "# Apply the following loop to one file at a time in filePath.\n",
    "for file in filePath:\n",
    "    \n",
    "    # Create a file name for a new output file.\n",
    "    \n",
    "    # First, get the existing file name \n",
    "    # (e.g. \"sessionlawsresol1955nort_0066.txt\")\n",
    "    # & remove the file extension.\n",
    "    outFileName = file.strip(\".txt\")\n",
    "    \n",
    "    # Then, concatenate (add) the existing file name with additional\n",
    "    # information (\"_noheader\") and the file extension to create a\n",
    "    # new name (e.g. \"sessionlawsresol1955nort_0066_noheader.txt\").\n",
    "    outFileName = outFileName + \"_corrected.txt\"\n",
    "\n",
    "    # Create and open a new \"outFile\" to save our results to.\n",
    "    # \"w\" tells Python that we plan to write to this file.\n",
    "    outFile = open(outFileName, \"w\")\n",
    "    \n",
    "    # Open a file in \"read\" (r) mode.\n",
    "    inFile = open(file, \"r\")\n",
    "    \n",
    "    # Read in the contents of that file.\n",
    "    inFile = inFile.read()\n",
    "    \n",
    "    # Search inFile for the the regular expression.\n",
    "    if re.search(regex_search, inFile):\n",
    "        \n",
    "        # If the regex search is found,\n",
    "        # print a statement to let us know that there is a match.\n",
    "        print(outFileName, \"Match found.\")\n",
    "        \n",
    "        # Substitute the regex_search for the replacement phrase\n",
    "        # and write the updated contents of inFile to outFile.\n",
    "        outFile.write(re.sub(regex_search, replacement, inFile))\n",
    "    \n",
    "    # If neither the regex search is not found,\n",
    "    else:\n",
    "        \n",
    "        # print a statement to let us know that no matches were found.\n",
    "        print(outFileName, \"No match found.\")\n",
    "        \n",
    "        # And write all of the contents from the inFile to the outFile.\n",
    "        outFile.write(inFile)\n",
    "    \n",
    "    # Close the current outFile and move to the next file.\n",
    "    outFile.close()\n",
    "    \n",
    "# The loop will finish when Python has gone through all files in \n",
    "# the sample_output folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before We Go\n",
    "---\n",
    "\n",
    "Post to Slack #intro-to-ocr 1-2 questions that have come up for you as a result of today's lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework <a class=\"anchor\" id=\"homework\">\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose one of the following projects:\n",
    "    - [Quantifying Kissinger](https://blog.quantifyingkissinger.com/)\n",
    "        - [Sources and Process](https://blog.quantifyingkissinger.com/category/methods/sources-and-process/)\n",
    "    - [TROVE](https://trove.nla.gov.au/)\n",
    "        - [OCR Overview](https://www.nla.gov.au/content/ocr-overview)\n",
    "        - [Blog post: Australian Newspapers Digitisation Program - The OCR process](https://www.nla.gov.au/stories/blog/behind-the-scenes/2010/05/28/australian-newspapers-digitisation-program-the-ocr-process)\n",
    "    - [Viral Texts](https://viraltexts.org/2015/05/22/computational-methods-for-uncovering-reprinted-texts-in-antebellum-newspapers/) -- skim and Command+F or Control+F for \"OCR\"\n",
    "\n",
    "\n",
    "2. Read through the project's methods.\n",
    "\n",
    "\n",
    "3. As you read, consider the following questions:\n",
    "    - What is the project's goal?\n",
    "    - How does the project team structure its OCR process?\n",
    "    - What is one useful step or approach that the team includes in their OCR process?\n",
    "    - What is one less useful step or approach that the team includes in their OCR process? Or, what is one step that is missing?\n",
    "    - How do the useful, less useful, and/or missing steps or approaches impact the project?\n",
    "\n",
    "\n",
    "4. Post reflections on these questions and the project you chose in Slack #intro-to-ocr."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
